{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-02 19:19:55.073396\n",
      "2021-12-02 19:31:14.177762\n"
     ]
    }
   ],
   "source": [
    "from labeler import *\n",
    "\n",
    "data = getLabels(\"JawClench_labels_Ansh_12-02-21-1918.txt\")\n",
    "startTime = data[0]\n",
    "labels = data[1]\n",
    "data = getData(\"OpenBCI-RAW-2021-12-02_19-19-53.txt\",labels,startTime)\n",
    "\n",
    "data2 = getLabels(\"JawClench_labels_Ansh_12-02-21-1930.txt\")\n",
    "startTime = data2[0]\n",
    "labels = data2[1]\n",
    "data2 = getData(\"OpenBCI-RAW-2021-12-02_19-31-12.txt\", labels, startTime)\n",
    "\n",
    "data = np.append(data, data2, axis=0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.71241043e+03 6.74333881e+02 9.04597652e+02 1.91071489e+03\n",
      " 1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "for i in range (len(data)):\n",
    "    for j in range(len(data[i])):\n",
    "        if j < len(data[i])-1:\n",
    "            data[i][j] = (float)(data[i][j])\n",
    "        else:\n",
    "            data[i][j] = (int)(data[i][j])\n",
    "data = np.array(data)\n",
    "print(data[1700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(data)//(batch_size_total)\n",
    "    data = data[:n_batches*batch_size_total, :]\n",
    "    X = data[:,:-1]\n",
    "    Y = np.array(data[:, -1], dtype=np.int32)\n",
    "    X = X.reshape((batch_size, n_batches*seq_length, 4))\n",
    "    Y = Y.reshape((batch_size, -1))\n",
    "\n",
    "    for n in range(0, Y.shape[1], seq_length):\n",
    "        x = X[:, n:n+seq_length]\n",
    "        y = Y[:, n:n+seq_length]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(data, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[[1615.87426451  475.56941827  807.67066438 1931.42645332]\n",
      "  [1614.01179433  474.96355447  813.60588561 1941.3895468 ]\n",
      "  [1649.01351801  490.22608619  826.41878286 1977.03079338]\n",
      "  [1651.81844298  479.26443942  812.64847122 1960.68743057]\n",
      "  [1651.89698088  472.18854871  802.00471595 1941.5915014 ]\n",
      "  [1609.17610368  491.38171528  826.86383095 1921.12302894]\n",
      "  [1571.44799293  489.05923741  818.68466975 1940.64156681]\n",
      "  [1518.60694646  474.27915278  805.15371171 1905.17983543]\n",
      "  [1512.89611923  491.90156137  819.87769784 1912.76809244]\n",
      "  [1556.76888561  500.28641707  833.50589327 1925.54359069]]]\n",
      "\n",
      "y\n",
      " [[0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:1, :10])\n",
    "print('\\ny\\n', y[:1, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_nodes, num_outputs, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_outputs=num_outputs\n",
    "        \n",
    "        self.lstm = nn.LSTM(num_nodes, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, num_outputs)\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = net.num_outputs\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            # x = one_hot_encode(x, n_chars)\n",
    "            x = np.array(x, dtype=np.float32)\n",
    "            inputs = torch.from_numpy(x).float()\n",
    "            targets =  torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # print(targets)\n",
    "            # print(targets.shape, output.shape)\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    # y = one_hot_encode(y, 2)\n",
    "                    x, y = torch.from_numpy(x).float(), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMNet(\n",
      "  (lstm): LSTM(4, 512, num_layers=2, batch_first=True, dropout=0.1)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden=128\n",
    "n_layers=3\n",
    "\n",
    "net = LSTMNet(4, 2, n_hidden, n_layers, drop_prob=0.1)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27059\n",
      "Epoch: 4/100... Step: 10... Loss: 0.6709... Val Loss: nan\n",
      "Epoch: 7/100... Step: 20... Loss: 0.5742... Val Loss: nan\n",
      "Epoch: 10/100... Step: 30... Loss: 0.5510... Val Loss: nan\n",
      "Epoch: 14/100... Step: 40... Loss: 0.5516... Val Loss: nan\n",
      "Epoch: 17/100... Step: 50... Loss: 0.6214... Val Loss: nan\n",
      "Epoch: 20/100... Step: 60... Loss: 0.5430... Val Loss: nan\n",
      "Epoch: 24/100... Step: 70... Loss: 0.6051... Val Loss: nan\n",
      "Epoch: 27/100... Step: 80... Loss: 0.5185... Val Loss: nan\n",
      "Epoch: 30/100... Step: 90... Loss: 0.4888... Val Loss: nan\n",
      "Epoch: 34/100... Step: 100... Loss: 0.5148... Val Loss: nan\n",
      "Epoch: 37/100... Step: 110... Loss: 0.4383... Val Loss: nan\n",
      "Epoch: 40/100... Step: 120... Loss: 0.4664... Val Loss: nan\n",
      "Epoch: 44/100... Step: 130... Loss: 0.5022... Val Loss: nan\n",
      "Epoch: 47/100... Step: 140... Loss: 0.5445... Val Loss: nan\n",
      "Epoch: 50/100... Step: 150... Loss: 0.4526... Val Loss: nan\n",
      "Epoch: 54/100... Step: 160... Loss: 0.5763... Val Loss: nan\n",
      "Epoch: 57/100... Step: 170... Loss: 0.5056... Val Loss: nan\n",
      "Epoch: 60/100... Step: 180... Loss: 0.5054... Val Loss: nan\n",
      "Epoch: 64/100... Step: 190... Loss: 0.5152... Val Loss: nan\n",
      "Epoch: 67/100... Step: 200... Loss: 0.4988... Val Loss: nan\n",
      "Epoch: 70/100... Step: 210... Loss: 0.4975... Val Loss: nan\n",
      "Epoch: 74/100... Step: 220... Loss: 0.5581... Val Loss: nan\n",
      "Epoch: 77/100... Step: 230... Loss: 0.4371... Val Loss: nan\n",
      "Epoch: 80/100... Step: 240... Loss: 0.7097... Val Loss: nan\n",
      "Epoch: 84/100... Step: 250... Loss: 0.5400... Val Loss: nan\n",
      "Epoch: 87/100... Step: 260... Loss: 0.5294... Val Loss: nan\n",
      "Epoch: 90/100... Step: 270... Loss: 0.4951... Val Loss: nan\n",
      "Epoch: 94/100... Step: 280... Loss: 0.5081... Val Loss: nan\n",
      "Epoch: 97/100... Step: 290... Loss: 0.4183... Val Loss: nan\n",
      "Epoch: 100/100... Step: 300... Loss: 0.4776... Val Loss: nan\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "seq_length = 555\n",
    "n_epochs = 100 # start smaller if you are just testing initial behavior\n",
    "print(len(data))\n",
    "# train the model\n",
    "train(net, data, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10, val_frac=0.0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f0226f8970b7fd921ddb6e667a73322eca69822d5a29d05eb027cdb50614cb4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
